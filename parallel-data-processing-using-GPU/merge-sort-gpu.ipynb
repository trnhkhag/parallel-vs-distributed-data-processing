{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt-get update\n!apt-get install -y build-essential nvidia-cuda-toolkit","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:53:23.519370Z","iopub.execute_input":"2025-03-06T13:53:23.519685Z","iopub.status.idle":"2025-03-06T13:53:29.071069Z","shell.execute_reply.started":"2025-03-06T13:53:23.519660Z","shell.execute_reply":"2025-03-06T13:53:29.070159Z"}},"outputs":[{"name":"stdout","text":"Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\nHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease                          \nHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease         \nHit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease                                      \nHit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease                                    \nGet:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]                                \nGet:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]                           \nHit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease               \nHit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\nHit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\nGet:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,730 kB]\nGet:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,669 kB]\nFetched 11.5 MB in 2s (6,681 kB/s)                          \nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nbuild-essential is already the newest version (12.9ubuntu3).\nnvidia-cuda-toolkit is already the newest version (11.5.1-1ubuntu1).\n0 upgraded, 0 newly installed, 0 to remove and 150 not upgraded.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Tạo file mã nguồn CUDA","metadata":{}},{"cell_type":"code","source":"%%writefile cuda_info.cu\n#include <iostream>\n#include <cuda_runtime.h>\nusing namespace std;\nint main(int argc, char ** argv)\n{\n    int deviceCount;\n    cudaGetDeviceCount(&deviceCount);\n    for (int dev = 0; dev < deviceCount; dev++)\n    {\n        cudaDeviceProp deviceProp;\n        cudaGetDeviceProperties(&deviceProp, dev);\n        if (dev == 0)\n        {\n            if (deviceProp.major == 9999 && deviceProp.minor == 9999)\n            {\n                cout << \"No CUDA GPU has been detected\" << endl;\n                return -1;\n            }\n            else if (deviceCount == 1)\n            {\n                cout << \"There is 1 device supporting CUDA\" << endl;\n            }\n            else\n            {\n                cout << \"There are \" << deviceCount << \" devices supporting CUDA\" << endl;\n            }\n        }\n        cout << \"Device \" << dev << \" name: \" << deviceProp.name << endl;\n        cout << \" Computational Capabilities: \" << deviceProp.major << \".\" << deviceProp.minor << endl;\n        cout << \" Maximum global memory size: \" << deviceProp.totalGlobalMem << endl;\n        cout << \" Maximum constant memory size: \" << deviceProp.totalConstMem << endl;\n        cout << \" Maximum shared memory size per block: \" << deviceProp.sharedMemPerBlock << endl;\n        cout << \" Maximum block dimensions: \" << deviceProp.maxThreadsDim[0] << \" x \" << deviceProp.maxThreadsDim[1] << \" x \" << deviceProp.maxThreadsDim[2] << endl;\n        cout << \" Maximum grid dimensions: \" << deviceProp.maxGridSize[0] << \" x \" << deviceProp.maxGridSize[1] << \" x \" << deviceProp.maxGridSize[2] << endl;\n        cout << \" Warp size: \" << deviceProp.warpSize << endl;\n    }\n    cudaDeviceReset();\n    return 0;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:53:35.764811Z","iopub.execute_input":"2025-03-06T13:53:35.765136Z","iopub.status.idle":"2025-03-06T13:53:35.772315Z","shell.execute_reply.started":"2025-03-06T13:53:35.765111Z","shell.execute_reply":"2025-03-06T13:53:35.771327Z"}},"outputs":[{"name":"stdout","text":"Overwriting cuda_info.cu\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Biên dịch mã CUDA","metadata":{}},{"cell_type":"code","source":"!nvcc cuda_info.cu -o cuda_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:53:41.400499Z","iopub.execute_input":"2025-03-06T13:53:41.400876Z","iopub.status.idle":"2025-03-06T13:53:43.367489Z","shell.execute_reply.started":"2025-03-06T13:53:41.400845Z","shell.execute_reply":"2025-03-06T13:53:43.366427Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!./cuda_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:53:45.138792Z","iopub.execute_input":"2025-03-06T13:53:45.139129Z","iopub.status.idle":"2025-03-06T13:53:45.318185Z","shell.execute_reply.started":"2025-03-06T13:53:45.139106Z","shell.execute_reply":"2025-03-06T13:53:45.317271Z"}},"outputs":[{"name":"stdout","text":"There are 2 devices supporting CUDA\nDevice 0 name: Tesla T4\n Computational Capabilities: 7.5\n Maximum global memory size: 15828320256\n Maximum constant memory size: 65536\n Maximum shared memory size per block: 49152\n Maximum block dimensions: 1024 x 1024 x 64\n Maximum grid dimensions: 2147483647 x 65535 x 65535\n Warp size: 32\nDevice 1 name: Tesla T4\n Computational Capabilities: 7.5\n Maximum global memory size: 15828320256\n Maximum constant memory size: 65536\n Maximum shared memory size per block: 49152\n Maximum block dimensions: 1024 x 1024 x 64\n Maximum grid dimensions: 2147483647 x 65535 x 65535\n Warp size: 32\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Merge Sort","metadata":{}},{"cell_type":"markdown","source":"### 1. Ý tưởng thuật toán","metadata":{}},{"cell_type":"code","source":"from IPython.display import display, Markdown\n\ndef print_formatted_output():\n    data = \"\"\"\n    |                   Input                   |\n    |-------------------------------------------|\n    |      8 3 1 9 1 2 7 5 9 3 6 4 2 0 2 5      |\n    |                                           |\n    |  Thread1 | Thread2  | Thread3  | Thread4  |\n    |----------|----------|----------|----------|\n    | 8 3 1 9  | 1 2 7 5  | 9 3 6 4  | 2 0 2 5  |\n    |  38 19   |  12 57   |  39 46   |  02 25   |\n    |---------------------|---------------------|\n    |       Thread1       |        Thread2      |\n    |   1398       1257   |   3469       0225   |\n    |       11235789      |       02234569      |\n    |-------------------------------------------|\n    |                  Thread1                  |\n    |      0 1 1 2 2 2 3 3 4 5 5 6 7 8 9 9      |\n    |                                           |\n    \"\"\"\n    display(Markdown(f'```\\n{data}\\n```'))\n\nprint_formatted_output()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Prepare input arrays","metadata":{}},{"cell_type":"code","source":"import random\n\ndef generate_merge_sort_input_file(filename, size):\n    \"\"\"\n    Generate an input file for merge sort with a 1D array of random integers.\n    - `filename`: Name of the output file.\n    - `size`: Number of elements in the array.\n    \"\"\"\n    with open(filename, 'w') as f:\n        # Write the size of the array\n        f.write(f\"{size}\\n\")\n        \n        # Generate a list of random integers (range: 0 to 9999)\n        numbers = [str(random.randint(0, 9999)) for _ in range(size)]\n        \n        # Write numbers to file\n        f.write(\" \".join(numbers) + \"\\n\")\n\n# List of sizes to generate input files\nsizes = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000]\n\n# Generate test cases for all sizes\nfor size in sizes:\n    filename = f\"MERGE_SORT_{size}.INP\"\n    generate_merge_sort_input_file(filename, size)\n\nprint(\"✅ Merge sort input files generated successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:53:55.839309Z","iopub.execute_input":"2025-03-06T13:53:55.839673Z","iopub.status.idle":"2025-03-06T13:53:57.597918Z","shell.execute_reply.started":"2025-03-06T13:53:55.839646Z","shell.execute_reply":"2025-03-06T13:53:57.596869Z"}},"outputs":[{"name":"stdout","text":"✅ Merge sort input files generated successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## CPU Merge Sort (Cython)","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\nimport cudf\nimport cupy as cp\nimport cython\nfrom numba import cuda, int32\nfrom timeit import default_timer as timer\nfrom scipy.sparse import lil_matrix\nfrom pyspark.sql import SparkSession\nfrom pyspark.mllib.linalg.distributed import *\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:54:02.573626Z","iopub.execute_input":"2025-03-06T13:54:02.574041Z","iopub.status.idle":"2025-03-06T13:54:04.757484Z","shell.execute_reply.started":"2025-03-06T13:54:02.574011Z","shell.execute_reply":"2025-03-06T13:54:04.756805Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"%load_ext cython","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:54:06.686330Z","iopub.execute_input":"2025-03-06T13:54:06.686907Z","iopub.status.idle":"2025-03-06T13:54:07.108071Z","shell.execute_reply.started":"2025-03-06T13:54:06.686865Z","shell.execute_reply":"2025-03-06T13:54:07.107331Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"%%cython\nimport numpy as np\ncimport numpy as np\ncimport cython\n\n@cython.boundscheck(False)  # Disable bounds checking for speed\n@cython.wraparound(False)   # Disable negative index wrapping\n@cython.cdivision(True)  # Enable C-style division\ncdef void _merge(np.ndarray[np.int32_t] arr, int left, int mid, int right):\n    cdef int n1 = mid - left + 1\n    cdef int n2 = right - mid\n    cdef int i, j, k\n\n    # Allocate memory for temporary arrays\n    cdef np.ndarray[np.int32_t] L = np.empty(n1, dtype=np.int32)\n    cdef np.ndarray[np.int32_t] R = np.empty(n2, dtype=np.int32)\n\n    # Copy data into temporary arrays\n    for i in range(n1):\n        L[i] = arr[left + i]\n    for j in range(n2):\n        R[j] = arr[mid + 1 + j]\n\n    # Merge temporary arrays back into arr[left:right+1]\n    i = 0\n    j = 0\n    k = left\n    while i < n1 and j < n2:\n        if L[i] <= R[j]:\n            arr[k] = L[i]\n            i += 1\n        else:\n            arr[k] = R[j]\n            j += 1\n        k += 1\n\n    # Copy remaining elements of L[], if any\n    while i < n1:\n        arr[k] = L[i]\n        i += 1\n        k += 1\n\n    # Copy remaining elements of R[], if any\n    while j < n2:\n        arr[k] = R[j]\n        j += 1\n        k += 1\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\n@cython.cdivision(True)\ncdef void _merge_sort(np.ndarray[np.int32_t] arr, int left, int right):\n    cdef int mid\n    if left < right:\n        mid = left + (right - left) // 2  # Compute middle index\n        _merge_sort(arr, left, mid)      # Recursively sort left half\n        _merge_sort(arr, mid + 1, right) # Recursively sort right half\n        _merge(arr, left, mid, right)    # Merge sorted halves\n\ndef cpu_merge_sort(np.ndarray[np.int32_t] arr):\n    \"\"\"\n    Sorts a NumPy array of integers using Merge Sort with Cython optimizations.\n    \"\"\"\n    cdef int n = arr.shape[0]\n    _merge_sort(arr, 0, n - 1)  # Call optimized merge sort\n    return arr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:54:28.713460Z","iopub.execute_input":"2025-03-06T13:54:28.713872Z","iopub.status.idle":"2025-03-06T13:54:29.191461Z","shell.execute_reply.started":"2025-03-06T13:54:28.713838Z","shell.execute_reply":"2025-03-06T13:54:29.190719Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nimport time\n\ndef read_input_file(filename):\n    \"\"\"\n    Reads an input file and returns the array of integers.\n    \"\"\"\n    with open(filename, 'r') as f:\n        size = int(f.readline().strip())  # Read size (not used in NumPy)\n        numbers = np.array(list(map(int, f.readline().strip().split())), dtype=np.int32)\n    return numbers\n\ndef write_output_file(filename, sorted_numbers):\n    \"\"\"\n    Writes the sorted array to an output file.\n    \"\"\"\n    with open(filename, 'w') as f:\n        f.write(\" \".join(map(str, sorted_numbers)) + \"\\n\")\n\n# List of sizes\nsizes = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000]\n\n# Process each input file\nfor size in sizes:\n    input_filename = f\"MERGE_SORT_{size}.INP\"\n    output_filename = f\"MERGE_SORT_{size}.OUT\"\n\n    print(f\"Processing {input_filename}...\")\n\n    # Read input file\n    arr = read_input_file(input_filename)\n\n    # Measure sorting time\n    start_time = time.time()\n    sorted_arr = cpu_merge_sort(arr)\n    end_time = time.time()\n\n    # Write output file\n    write_output_file(output_filename, sorted_arr)\n\n    print(f\"✅ {output_filename} saved! Sorting time: {(end_time - start_time) * 1000:.3f} ms\")\n\nprint(\"✅ All files processed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:54:32.425714Z","iopub.execute_input":"2025-03-06T13:54:32.426130Z","iopub.status.idle":"2025-03-06T13:54:36.032370Z","shell.execute_reply.started":"2025-03-06T13:54:32.426101Z","shell.execute_reply":"2025-03-06T13:54:36.031436Z"}},"outputs":[{"name":"stdout","text":"Processing MERGE_SORT_100.INP...\n✅ MERGE_SORT_100.OUT saved! Sorting time: 0.295 ms\nProcessing MERGE_SORT_500.INP...\n✅ MERGE_SORT_500.OUT saved! Sorting time: 1.144 ms\nProcessing MERGE_SORT_1000.INP...\n✅ MERGE_SORT_1000.OUT saved! Sorting time: 1.911 ms\nProcessing MERGE_SORT_5000.INP...\n✅ MERGE_SORT_5000.OUT saved! Sorting time: 9.478 ms\nProcessing MERGE_SORT_10000.INP...\n✅ MERGE_SORT_10000.OUT saved! Sorting time: 17.058 ms\nProcessing MERGE_SORT_50000.INP...\n✅ MERGE_SORT_50000.OUT saved! Sorting time: 86.552 ms\nProcessing MERGE_SORT_100000.INP...\n✅ MERGE_SORT_100000.OUT saved! Sorting time: 168.884 ms\nProcessing MERGE_SORT_500000.INP...\n✅ MERGE_SORT_500000.OUT saved! Sorting time: 828.747 ms\nProcessing MERGE_SORT_1000000.INP...\n✅ MERGE_SORT_1000000.OUT saved! Sorting time: 1596.694 ms\n✅ All files processed successfully!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### 3. CUDA Merge Sort","metadata":{}},{"cell_type":"code","source":"%%writefile mergeSortCUDA.cu\n#include <iostream>\n#include <fstream>\n#include <cuda_runtime.h>\n#include <vector>\n#include <string>\n\n#define THREADS 256  // Number of threads per block\n\nusing namespace std;\n\n// Device function to merge two sorted subarrays\n__device__ void merge(float* data, float* temp, int left, int mid, int right) {\n    int i = left, j = mid, k = left;\n    \n    while (i < mid && j < right) {\n        if (data[i] < data[j])\n            temp[k++] = data[i++];\n        else\n            temp[k++] = data[j++];\n    }\n    \n    while (i < mid)\n        temp[k++] = data[i++];\n    while (j < right)\n        temp[k++] = data[j++];\n    \n    // Copy sorted elements back to the original array\n    for (int i = left; i < right; i++)\n        data[i] = temp[i];\n}\n\n// Kernel function for parallel merge sort\n__global__ void mergeSortKernel(float* data, float* temp, int size, int width) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int left = idx * width * 2;\n    \n    if (left >= size) return; // Out of bounds\n    \n    int mid = min(left + width, size);\n    int right = min(left + 2 * width, size);\n    \n    if (mid < right)\n        merge(data, temp, left, mid, right);\n}\n\n// Host function for iterative parallel merge sort\nvoid mergeSort(float* deviceData, int size) {\n    float* deviceTemp;\n    cudaMalloc((void**)&deviceTemp, sizeof(float) * size);\n    \n    for (int width = 1; width < size; width *= 2) {\n        int numBlocks = (size / (2 * width) + THREADS - 1) / THREADS;\n        mergeSortKernel<<<numBlocks, THREADS>>>(deviceData, deviceTemp, size, width);\n        cudaDeviceSynchronize();\n    }\n    \n    cudaFree(deviceTemp);\n}\n\nvoid processFile(const string& inputFile, const string& outputFile) {\n    float *hostData, *deviceData;\n    int size;\n\n    // CUDA events for timing\n    cudaEvent_t start, stop, start_total, stop_total;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n    cudaEventCreate(&start_total);\n    cudaEventCreate(&stop_total);\n    float milliseconds, total_time;\n\n    /* Importing data and creating host memory */\n    ifstream finp(inputFile);\n    cudaEventRecord(start);\n    \n    finp >> size;\n    int byteSize = sizeof(float) * size;\n    hostData = (float*)malloc(byteSize);\n    \n    for (int i = 0; i < size; i++) {\n        finp >> hostData[i];\n    }\n    finp.close();\n\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"[INFO] Time for importing data: \" << milliseconds << \" ms\" << endl;\n    cout << \"[INFO] Processing File: \" << inputFile << \", Array Size: \" << size << endl;\n\n    /* Start measuring total time */\n    cudaEventRecord(start_total);\n\n    /* Allocating GPU memory */\n    cudaEventRecord(start);\n    cudaMalloc((void**)&deviceData, byteSize);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"[INFO] Time for allocating GPU memory: \" << milliseconds << \" ms\" << endl;\n\n    /* Copying input memory to the GPU */\n    cudaEventRecord(start);\n    cudaMemcpy(deviceData, hostData, byteSize, cudaMemcpyHostToDevice);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"[INFO] Time to copy data from CPU to GPU: \" << milliseconds << \" ms\" << endl;\n\n    /* Running Merge Sort on GPU */\n    cudaEventRecord(start);\n    mergeSort(deviceData, size);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"[INFO] Time for kernel computation on GPU: \" << milliseconds << \" ms\" << endl;\n\n    /* Copying output memory to the CPU */\n    cudaEventRecord(start);\n    cudaMemcpy(hostData, deviceData, byteSize, cudaMemcpyDeviceToHost);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"[INFO] Time to copy data from GPU to CPU: \" << milliseconds << \" ms\" << endl;\n\n    /* End measuring total time */\n    cudaEventRecord(stop_total);\n    cudaEventSynchronize(stop_total);\n    cudaEventElapsedTime(&total_time, start_total, stop_total);\n    cout << \"[INFO] Total time: \" << total_time << \" ms\" << endl;\n\n    /* Output results */\n    ofstream fout(outputFile);\n    fout << size << endl;\n    for (int i = 0; i < size; i++) {\n        fout << hostData[i] << \" \";\n    }\n    fout.close();\n    cout << \"[INFO] Output written to \" << outputFile << endl;\n\n    /* Freeing GPU Memory */\n    cudaFree(deviceData);\n    free(hostData);\n\n    /* Destroy CUDA events */\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n    cudaEventDestroy(start_total);\n    cudaEventDestroy(stop_total);\n}\n\nint main() {\n    vector<string> inputFiles = {\n        \"MERGE_SORT_100.INP\",\n        \"MERGE_SORT_500.INP\",\n        \"MERGE_SORT_1000.INP\",\n        \"MERGE_SORT_5000.INP\",\n        \"MERGE_SORT_10000.INP\",\n        \"MERGE_SORT_50000.INP\",\n        \"MERGE_SORT_100000.INP\",\n        \"MERGE_SORT_500000.INP\",\n        \"MERGE_SORT_1000000.INP\"\n    };\n\n    for (const auto& inputFile : inputFiles) {\n        string outputFile = inputFile.substr(0, inputFile.find(\".\")) + \".OUT\";\n        processFile(inputFile, outputFile);\n    }\n\n    return 0;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:54:41.391238Z","iopub.execute_input":"2025-03-06T13:54:41.391569Z","iopub.status.idle":"2025-03-06T13:54:41.398139Z","shell.execute_reply.started":"2025-03-06T13:54:41.391535Z","shell.execute_reply":"2025-03-06T13:54:41.397129Z"}},"outputs":[{"name":"stdout","text":"Overwriting mergeSortCUDA.cu\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!nvcc mergeSortCUDA.cu -o mergeSort","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:54:47.983747Z","iopub.execute_input":"2025-03-06T13:54:47.984132Z","iopub.status.idle":"2025-03-06T13:54:50.314032Z","shell.execute_reply.started":"2025-03-06T13:54:47.984106Z","shell.execute_reply":"2025-03-06T13:54:50.312658Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!./mergeSort","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:54:53.280191Z","iopub.execute_input":"2025-03-06T13:54:53.280545Z","iopub.status.idle":"2025-03-06T13:54:55.209889Z","shell.execute_reply.started":"2025-03-06T13:54:53.280514Z","shell.execute_reply":"2025-03-06T13:54:55.208036Z"}},"outputs":[{"name":"stdout","text":"[INFO] Time for importing data: 0.065728 ms\n[INFO] Processing File: MERGE_SORT_100.INP, Array Size: 100\n[INFO] Time for allocating GPU memory: 0.178144 ms\n[INFO] Time to copy data from CPU to GPU: 0.095968 ms\n[INFO] Time for kernel computation on GPU: 0.434688 ms\n[INFO] Time to copy data from GPU to CPU: 0.023872 ms\n[INFO] Total time: 0.831808 ms\n[INFO] Output written to MERGE_SORT_100.OUT\n[INFO] Time for importing data: 0.18992 ms\n[INFO] Processing File: MERGE_SORT_500.INP, Array Size: 500\n[INFO] Time for allocating GPU memory: 0.172416 ms\n[INFO] Time to copy data from CPU to GPU: 0.017024 ms\n[INFO] Time for kernel computation on GPU: 0.276672 ms\n[INFO] Time to copy data from GPU to CPU: 0.019264 ms\n[INFO] Total time: 0.56816 ms\n[INFO] Output written to MERGE_SORT_500.OUT\n[INFO] Time for importing data: 0.176672 ms\n[INFO] Processing File: MERGE_SORT_1000.INP, Array Size: 1000\n[INFO] Time for allocating GPU memory: 0.147264 ms\n[INFO] Time to copy data from CPU to GPU: 0.016512 ms\n[INFO] Time for kernel computation on GPU: 0.474464 ms\n[INFO] Time to copy data from GPU to CPU: 0.01952 ms\n[INFO] Total time: 0.739808 ms\n[INFO] Output written to MERGE_SORT_1000.OUT\n[INFO] Time for importing data: 0.979584 ms\n[INFO] Processing File: MERGE_SORT_5000.INP, Array Size: 5000\n[INFO] Time for allocating GPU memory: 0.1672 ms\n[INFO] Time to copy data from CPU to GPU: 0.021664 ms\n[INFO] Time for kernel computation on GPU: 2.76707 ms\n[INFO] Time to copy data from GPU to CPU: 0.019808 ms\n[INFO] Total time: 3.07443 ms\n[INFO] Output written to MERGE_SORT_5000.OUT\n[INFO] Time for importing data: 2.05315 ms\n[INFO] Processing File: MERGE_SORT_10000.INP, Array Size: 10000\n[INFO] Time for allocating GPU memory: 0.147968 ms\n[INFO] Time to copy data from CPU to GPU: 0.027264 ms\n[INFO] Time for kernel computation on GPU: 5.73027 ms\n[INFO] Time to copy data from GPU to CPU: 0.032384 ms\n[INFO] Total time: 6.00678 ms\n[INFO] Output written to MERGE_SORT_10000.OUT\n[INFO] Time for importing data: 9.78755 ms\n[INFO] Processing File: MERGE_SORT_50000.INP, Array Size: 50000\n[INFO] Time for allocating GPU memory: 0.164416 ms\n[INFO] Time to copy data from CPU to GPU: 0.07744 ms\n[INFO] Time for kernel computation on GPU: 25.4596 ms\n[INFO] Time to copy data from GPU to CPU: 0.061376 ms\n[INFO] Total time: 25.8742 ms\n[INFO] Output written to MERGE_SORT_50000.OUT\n[INFO] Time for importing data: 20.2253 ms\n[INFO] Processing File: MERGE_SORT_100000.INP, Array Size: 100000\n[INFO] Time for allocating GPU memory: 0.154496 ms\n[INFO] Time to copy data from CPU to GPU: 0.133856 ms\n[INFO] Time for kernel computation on GPU: 50.8409 ms\n[INFO] Time to copy data from GPU to CPU: 0.091616 ms\n[INFO] Total time: 51.3044 ms\n[INFO] Output written to MERGE_SORT_100000.OUT\n[INFO] Time for importing data: 96.5316 ms\n[INFO] Processing File: MERGE_SORT_500000.INP, Array Size: 500000\n[INFO] Time for allocating GPU memory: 0.183424 ms\n[INFO] Time to copy data from CPU to GPU: 0.616 ms\n[INFO] Time for kernel computation on GPU: 217.575 ms\n[INFO] Time to copy data from GPU to CPU: 0.478176 ms\n[INFO] Total time: 218.977 ms\n[INFO] Output written to MERGE_SORT_500000.OUT\n[INFO] Time for importing data: 199.223 ms\n[INFO] Processing File: MERGE_SORT_1000000.INP, Array Size: 1000000\n[INFO] Time for allocating GPU memory: 0.23232 ms\n[INFO] Time to copy data from CPU to GPU: 0.983712 ms\n[INFO] Time for kernel computation on GPU: 242.396 ms\n[INFO] Time to copy data from GPU to CPU: 0.912128 ms\n[INFO] Total time: 244.668 ms\n[INFO] Output written to MERGE_SORT_1000000.OUT\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### 4. Cupy Merge Sort","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cupy as cp\nimport time\nimport os\n\n# Define CUDA kernel for merging sorted subarrays\nmerge_kernel = r'''\nextern \"C\" __global__ void merge(\n    float* data, float* temp, int size, int width) {\n    \n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int left = idx * width * 2;\n    \n    if (left >= size) return; // Out of bounds\n    \n    int mid = min(left + width, size);\n    int right = min(left + 2 * width, size);\n    \n    int i = left, j = mid, k = left;\n    \n    while (i < mid && j < right) {\n        if (data[i] < data[j])\n            temp[k++] = data[i++];\n        else\n            temp[k++] = data[j++];\n    }\n    \n    while (i < mid)\n        temp[k++] = data[i++];\n    while (j < right)\n        temp[k++] = data[j++];\n    \n    for (int i = left; i < right; i++)\n        data[i] = temp[i];\n}\n'''\n\ndef merge_sort_gpu(data):\n    \"\"\" Perform parallel merge sort using CUDA with CuPy. \"\"\"\n    if isinstance(data, np.ndarray):\n        data = cp.asarray(data, dtype=cp.float32)\n\n    size = data.size\n    temp = cp.zeros_like(data, dtype=cp.float32)\n\n    # Compile CUDA kernel\n    kernel = cp.RawKernel(merge_kernel, 'merge')\n\n    # Configure grid & block size\n    threads_per_block = 256\n    for width in [2**i for i in range(max(1, int(cp.log2(size)) + 1))]:\n        num_blocks = (size // (2 * width) + threads_per_block - 1) // threads_per_block\n        kernel((num_blocks,), (threads_per_block,), (data, temp, size, width))\n        cp.cuda.stream.get_current_stream().synchronize()\n\n    del temp  # Free temporary GPU memory\n    cp.get_default_memory_pool().free_all_blocks()  # Force GPU memory cleanup\n\n    return data\n\nif __name__ == \"__main__\":\n    input_files = [\n        \"MERGE_SORT_100.INP\",\n        \"MERGE_SORT_500.INP\",\n        \"MERGE_SORT_1000.INP\",\n        \"MERGE_SORT_5000.INP\",\n        \"MERGE_SORT_10000.INP\",\n        \"MERGE_SORT_50000.INP\",\n        \"MERGE_SORT_100000.INP\",\n        \"MERGE_SORT_500000.INP\",\n        \"MERGE_SORT_1000000.INP\"\n    ]\n\n    for input_file in input_files:\n        output_file = input_file.replace(\".INP\", \".OUT\")\n\n        if not os.path.exists(input_file):\n            print(f\"Skipping {input_file} (File not found)\")\n            continue\n\n        print(f\"\\nProcessing {input_file}...\")\n\n        # Load data\n        with open(input_file, \"r\") as f:\n            size = int(f.readline().strip())\n            array = np.array(list(map(float, f.readline().split())), dtype=np.float32)\n\n        print(f\"Array Size: {size}\")\n\n        # Start total timing\n        start_total = time.time()\n\n        # Copy to GPU\n        start = time.time()\n        data_gpu = cp.asarray(array)\n        cp.cuda.stream.get_current_stream().synchronize()\n        end = time.time()\n        print(f\"Time to copy data to GPU: {(end - start) * 1000:.4f} ms\")\n\n        # Sort the array\n        start = time.time()\n        sorted_gpu = merge_sort_gpu(data_gpu)\n        cp.cuda.stream.get_current_stream().synchronize()\n        end = time.time()\n        print(f\"Time for kernel computation on GPU: {(end - start) * 1000:.4f} ms\")\n\n        # Copy back to CPU\n        start = time.time()\n        sorted_array = cp.asnumpy(sorted_gpu)\n        cp.cuda.stream.get_current_stream().synchronize()\n        end = time.time()\n        print(f\"Time to copy data from GPU: {(end - start) * 1000:.4f} ms\")\n\n        # Free GPU memory\n        del data_gpu, sorted_gpu\n        cp.get_default_memory_pool().free_all_blocks()\n\n        end_total = time.time()\n        print(f\"Total time: {(end_total - start_total) * 1000:.4f} ms\")\n\n        # Save output\n        with open(output_file, \"w\") as f:\n            f.write(f\"{size}\\n\")\n            f.write(\" \".join(map(str, sorted_array)) + \"\\n\")\n\n        print(f\"Output saved to {output_file}\")\n\n    print(\"\\nAll input files processed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T14:05:26.919443Z","iopub.execute_input":"2025-03-06T14:05:26.919920Z","iopub.status.idle":"2025-03-06T14:05:28.757142Z","shell.execute_reply.started":"2025-03-06T14:05:26.919868Z","shell.execute_reply":"2025-03-06T14:05:28.756137Z"}},"outputs":[{"name":"stdout","text":"\nProcessing MERGE_SORT_100.INP...\nArray Size: 100\nTime to copy data to GPU: 1.2681 ms\nTime for kernel computation on GPU: 0.9990 ms\nTime to copy data from GPU: 0.3119 ms\nTotal time: 2.7633 ms\nOutput saved to MERGE_SORT_100.OUT\n\nProcessing MERGE_SORT_500.INP...\nArray Size: 500\nTime to copy data to GPU: 0.5169 ms\nTime for kernel computation on GPU: 1.0090 ms\nTime to copy data from GPU: 0.0579 ms\nTotal time: 1.9238 ms\nOutput saved to MERGE_SORT_500.OUT\n\nProcessing MERGE_SORT_1000.INP...\nArray Size: 1000\nTime to copy data to GPU: 0.3240 ms\nTime for kernel computation on GPU: 1.3871 ms\nTime to copy data from GPU: 0.2453 ms\nTotal time: 2.1415 ms\nOutput saved to MERGE_SORT_1000.OUT\n\nProcessing MERGE_SORT_5000.INP...\nArray Size: 5000\nTime to copy data to GPU: 0.3104 ms\nTime for kernel computation on GPU: 4.8506 ms\nTime to copy data from GPU: 0.3037 ms\nTotal time: 5.7735 ms\nOutput saved to MERGE_SORT_5000.OUT\n\nProcessing MERGE_SORT_10000.INP...\nArray Size: 10000\nTime to copy data to GPU: 0.2785 ms\nTime for kernel computation on GPU: 9.6552 ms\nTime to copy data from GPU: 0.2422 ms\nTotal time: 10.3111 ms\nOutput saved to MERGE_SORT_10000.OUT\n\nProcessing MERGE_SORT_50000.INP...\nArray Size: 50000\nTime to copy data to GPU: 0.3939 ms\nTime for kernel computation on GPU: 40.8518 ms\nTime to copy data from GPU: 0.0949 ms\nTotal time: 41.4622 ms\nOutput saved to MERGE_SORT_50000.OUT\n\nProcessing MERGE_SORT_100000.INP...\nArray Size: 100000\nTime to copy data to GPU: 0.6015 ms\nTime for kernel computation on GPU: 81.4674 ms\nTime to copy data from GPU: 0.3524 ms\nTotal time: 82.5858 ms\nOutput saved to MERGE_SORT_100000.OUT\n\nProcessing MERGE_SORT_500000.INP...\nArray Size: 500000\nTime to copy data to GPU: 1.1773 ms\nTime for kernel computation on GPU: 256.8896 ms\nTime to copy data from GPU: 0.6063 ms\nTotal time: 258.9030 ms\nOutput saved to MERGE_SORT_500000.OUT\n\nProcessing MERGE_SORT_1000000.INP...\nArray Size: 1000000\nTime to copy data to GPU: 2.5153 ms\nTime for kernel computation on GPU: 365.5524 ms\nTime to copy data from GPU: 2.6560 ms\nTotal time: 371.1629 ms\nOutput saved to MERGE_SORT_1000000.OUT\n\nAll input files processed successfully!\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### 5. RAPIDS Merge Sort Cupy kernel","metadata":{}},{"cell_type":"code","source":"import cupy as cp\nimport cudf\nimport time\nimport os\n\n# CUDA Kernel for Merge Sort\nmerge_sort_kernel = cp.RawKernel(r'''\nextern \"C\" __global__ void merge_sort(int* arr, int* temp, int n, int width) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int left = tid * 2 * width;\n    int mid = min(left + width, n);\n    int right = min(left + 2 * width, n);\n\n    if (mid >= right) return;\n\n    int i = left, j = mid, k = left;\n    while (i < mid && j < right) {\n        if (arr[i] <= arr[j]) {\n            temp[k++] = arr[i++];\n        } else {\n            temp[k++] = arr[j++];\n        }\n    }\n    while (i < mid) temp[k++] = arr[i++];\n    while (j < right) temp[k++] = arr[j++];\n\n    for (int l = left; l < right; l++) {\n        arr[l] = temp[l];\n    }\n}\n''', 'merge_sort')\n\n# Custom GPU Merge Sort Function\ndef gpu_merge_sort(self):\n    if self.shape[1] != 1:\n        raise ValueError(\"Merge sort only supports single-column cuDF DataFrames\")\n\n    total_start = time.time()\n\n    # 1. Memory Allocation (GPU) - Fixed to 1D\n    start_alloc = time.time()\n    arr_gpu = cp.empty(self.shape[0], dtype=cp.int32)  # ✅ Ensure it's 1D\n    temp_gpu = cp.empty(self.shape[0], dtype=cp.int32)\n    end_alloc = time.time()\n\n    # 2. Copy Data from CPU to GPU\n    start_copy_h2d = time.time()\n    arr_gpu[:] = cp.asarray(self.iloc[:, 0].to_cupy(), dtype=cp.int32)  # ✅ Correct extraction\n    end_copy_h2d = time.time()\n\n    # 3. Kernel Execution (Iterative Merge Sort)\n    start_kernel = time.time()\n    n = arr_gpu.size\n    threads_per_block = 256\n    blocks_per_grid = (n + threads_per_block - 1) // threads_per_block\n\n    width = 1\n    while width < n:\n        grid_size = (n // (2 * width) + (1 if n % (2 * width) != 0 else 0))  # Prevent excessive blocks\n        merge_sort_kernel((grid_size,), (threads_per_block,), (arr_gpu, temp_gpu, n, width))\n        cp.cuda.Device(0).synchronize()\n        width *= 2\n    end_kernel = time.time()\n\n    # 4. Copy Data from GPU to CPU\n    start_copy_d2h = time.time()\n    sorted_result = arr_gpu.get()\n    end_copy_d2h = time.time()\n\n    # Total execution time\n    total_end = time.time()\n\n    # Convert back to cuDF DataFrame\n    sorted_df = cudf.DataFrame(sorted_result, columns=self.columns)\n\n    # Convert all times to milliseconds\n    alloc_time = (end_alloc - start_alloc) * 1000\n    copy_h2d_time = (end_copy_h2d - start_copy_h2d) * 1000\n    kernel_time = (end_kernel - start_kernel) * 1000\n    copy_d2h_time = (end_copy_d2h - start_copy_d2h) * 1000\n    total_time = (total_end - total_start) * 1000\n\n    # Print Timing Information\n    print(f\"Memory Allocation Time: {alloc_time:.4f} ms\")\n    print(f\"CPU → GPU Copy Time: {copy_h2d_time:.4f} ms\")\n    print(f\"Kernel Execution Time: {kernel_time:.4f} ms\")\n    print(f\"GPU → CPU Copy Time: {copy_d2h_time:.4f} ms\")\n    print(f\"Total Execution Time: {total_time:.4f} ms\")\n\n    return sorted_df\n\n# Monkey-patch cuDF DataFrame to add the custom method\ncudf.DataFrame.gpu_merge_sort = gpu_merge_sort\n\n# List of input files\ninput_files = [\n    \"MERGE_SORT_100.INP\",\n    \"MERGE_SORT_500.INP\",\n    \"MERGE_SORT_1000.INP\",\n    \"MERGE_SORT_5000.INP\",\n    \"MERGE_SORT_10000.INP\",\n    \"MERGE_SORT_50000.INP\",\n    \"MERGE_SORT_100000.INP\",\n    \"MERGE_SORT_500000.INP\",\n    \"MERGE_SORT_1000000.INP\"\n]\n\nif __name__ == \"__main__\":\n    for input_file in input_files:\n        output_file = input_file.replace(\".INP\", \".OUT\")\n\n        if not os.path.exists(input_file):\n            print(f\"Skipping {input_file} (File not found)\")\n            continue\n\n        print(f\"\\nProcessing {input_file}...\")\n\n        # Load data\n        with open(input_file, \"r\") as f:\n            size = int(f.readline().strip())\n            array = list(map(int, f.readline().split()))\n\n        print(f\"Array Size: {size}\")\n\n        # Convert to cuDF DataFrame\n        df = cudf.DataFrame({'values': array})\n\n        # Sort using GPU Merge Sort\n        sorted_df = df.gpu_merge_sort()\n\n        # Save output\n        sorted_array = sorted_df['values'].to_pandas().to_numpy()\n\n        with open(output_file, \"w\") as f:\n            f.write(f\"{size}\\n\")\n            f.write(\" \".join(map(str, sorted_array)) + \"\\n\")\n\n        print(f\"Output saved to {output_file}\")\n\n    print(\"\\nAll input files processed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:56:48.539951Z","iopub.execute_input":"2025-03-06T13:56:48.540379Z","iopub.status.idle":"2025-03-06T13:56:50.255434Z","shell.execute_reply.started":"2025-03-06T13:56:48.540347Z","shell.execute_reply":"2025-03-06T13:56:50.254585Z"}},"outputs":[{"name":"stdout","text":"\nProcessing MERGE_SORT_100.INP...\nArray Size: 100\nMemory Allocation Time: 0.2778 ms\nCPU → GPU Copy Time: 1.1435 ms\nKernel Execution Time: 1.2572 ms\nGPU → CPU Copy Time: 0.0675 ms\nTotal Execution Time: 2.7473 ms\nOutput saved to MERGE_SORT_100.OUT\n\nProcessing MERGE_SORT_500.INP...\nArray Size: 500\nMemory Allocation Time: 0.2205 ms\nCPU → GPU Copy Time: 0.9499 ms\nKernel Execution Time: 0.4997 ms\nGPU → CPU Copy Time: 0.0472 ms\nTotal Execution Time: 1.7183 ms\nOutput saved to MERGE_SORT_500.OUT\n\nProcessing MERGE_SORT_1000.INP...\nArray Size: 1000\nMemory Allocation Time: 0.1273 ms\nCPU → GPU Copy Time: 0.8874 ms\nKernel Execution Time: 0.7994 ms\nGPU → CPU Copy Time: 0.0455 ms\nTotal Execution Time: 1.8604 ms\nOutput saved to MERGE_SORT_1000.OUT\n\nProcessing MERGE_SORT_5000.INP...\nArray Size: 5000\nMemory Allocation Time: 0.1366 ms\nCPU → GPU Copy Time: 0.8128 ms\nKernel Execution Time: 4.1521 ms\nGPU → CPU Copy Time: 0.0620 ms\nTotal Execution Time: 5.1641 ms\nOutput saved to MERGE_SORT_5000.OUT\n\nProcessing MERGE_SORT_10000.INP...\nArray Size: 10000\nMemory Allocation Time: 0.1638 ms\nCPU → GPU Copy Time: 0.8521 ms\nKernel Execution Time: 8.8499 ms\nGPU → CPU Copy Time: 0.0629 ms\nTotal Execution Time: 9.9304 ms\nOutput saved to MERGE_SORT_10000.OUT\n\nProcessing MERGE_SORT_50000.INP...\nArray Size: 50000\nMemory Allocation Time: 0.1419 ms\nCPU → GPU Copy Time: 0.8154 ms\nKernel Execution Time: 40.7455 ms\nGPU → CPU Copy Time: 0.1016 ms\nTotal Execution Time: 41.8048 ms\nOutput saved to MERGE_SORT_50000.OUT\n\nProcessing MERGE_SORT_100000.INP...\nArray Size: 100000\nMemory Allocation Time: 0.1369 ms\nCPU → GPU Copy Time: 0.9046 ms\nKernel Execution Time: 81.3148 ms\nGPU → CPU Copy Time: 0.1817 ms\nTotal Execution Time: 82.5391 ms\nOutput saved to MERGE_SORT_100000.OUT\n\nProcessing MERGE_SORT_500000.INP...\nArray Size: 500000\nMemory Allocation Time: 0.5014 ms\nCPU → GPU Copy Time: 1.3366 ms\nKernel Execution Time: 259.2683 ms\nGPU → CPU Copy Time: 0.5929 ms\nTotal Execution Time: 261.7009 ms\nOutput saved to MERGE_SORT_500000.OUT\n\nProcessing MERGE_SORT_1000000.INP...\nArray Size: 1000000\nMemory Allocation Time: 0.4885 ms\nCPU → GPU Copy Time: 1.4949 ms\nKernel Execution Time: 365.8571 ms\nGPU → CPU Copy Time: 1.0784 ms\nTotal Execution Time: 368.9203 ms\nOutput saved to MERGE_SORT_1000000.OUT\n\nAll input files processed successfully!\n","output_type":"stream"}],"execution_count":16}]}