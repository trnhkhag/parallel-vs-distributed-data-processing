{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt-get update\n!apt-get install -y build-essential nvidia-cuda-toolkit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T09:59:08.115819Z","iopub.execute_input":"2025-03-07T09:59:08.116103Z","iopub.status.idle":"2025-03-07T10:00:40.000257Z","shell.execute_reply.started":"2025-03-07T09:59:08.116080Z","shell.execute_reply":"2025-03-07T10:00:39.999304Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tạo file mã nguồn CUDA","metadata":{}},{"cell_type":"code","source":"%%writefile cuda_info.cu\n#include <iostream>\n#include <cuda_runtime.h>\nusing namespace std;\nint main(int argc, char ** argv)\n{\n    int deviceCount;\n    cudaGetDeviceCount(&deviceCount);\n    for (int dev = 0; dev < deviceCount; dev++)\n    {\n        cudaDeviceProp deviceProp;\n        cudaGetDeviceProperties(&deviceProp, dev);\n        if (dev == 0)\n        {\n            if (deviceProp.major == 9999 && deviceProp.minor == 9999)\n            {\n                cout << \"No CUDA GPU has been detected\" << endl;\n                return -1;\n            }\n            else if (deviceCount == 1)\n            {\n                cout << \"There is 1 device supporting CUDA\" << endl;\n            }\n            else\n            {\n                cout << \"There are \" << deviceCount << \" devices supporting CUDA\" << endl;\n            }\n        }\n        cout << \"Device \" << dev << \" name: \" << deviceProp.name << endl;\n        cout << \" Computational Capabilities: \" << deviceProp.major << \".\" << deviceProp.minor << endl;\n        cout << \" Maximum global memory size: \" << deviceProp.totalGlobalMem << endl;\n        cout << \" Maximum constant memory size: \" << deviceProp.totalConstMem << endl;\n        cout << \" Maximum shared memory size per block: \" << deviceProp.sharedMemPerBlock << endl;\n        cout << \" Maximum block dimensions: \" << deviceProp.maxThreadsDim[0] << \" x \" << deviceProp.maxThreadsDim[1] << \" x \" << deviceProp.maxThreadsDim[2] << endl;\n        cout << \" Maximum grid dimensions: \" << deviceProp.maxGridSize[0] << \" x \" << deviceProp.maxGridSize[1] << \" x \" << deviceProp.maxGridSize[2] << endl;\n        cout << \" Warp size: \" << deviceProp.warpSize << endl;\n    }\n    cudaDeviceReset();\n    return 0;\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T10:01:43.122583Z","iopub.execute_input":"2025-03-07T10:01:43.122918Z","iopub.status.idle":"2025-03-07T10:01:43.129218Z","shell.execute_reply.started":"2025-03-07T10:01:43.122893Z","shell.execute_reply":"2025-03-07T10:01:43.128394Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Biên dịch mã CUDA","metadata":{}},{"cell_type":"code","source":"!nvcc cuda_info.cu -o cuda_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T10:01:46.653537Z","iopub.execute_input":"2025-03-07T10:01:46.653841Z","iopub.status.idle":"2025-03-07T10:01:49.380843Z","shell.execute_reply.started":"2025-03-07T10:01:46.653814Z","shell.execute_reply":"2025-03-07T10:01:49.379684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!./cuda_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T10:01:50.710629Z","iopub.execute_input":"2025-03-07T10:01:50.710913Z","iopub.status.idle":"2025-03-07T10:01:50.906741Z","shell.execute_reply.started":"2025-03-07T10:01:50.710890Z","shell.execute_reply":"2025-03-07T10:01:50.905926Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Matrix Multiplication","metadata":{}},{"cell_type":"markdown","source":"# Generate input file ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef generate_matrix_input_file(filename, m, k, n):\n    \"\"\"\n    Generate an input file for matrix multiplication:\n    - Matrix A of size m x k\n    - Matrix B of size k x n\n    - Resulting matrix C will be of size m x n\n    \"\"\"\n    with open(filename, 'w') as f:\n        # Write size of matrix A\n        f.write(f\"{m} {k}\\n\")\n\n        # Generate and write matrix A (random integers between 0 and 100)\n        A = np.random.randint(0, 101, size=(m, k))\n        np.savetxt(f, A, fmt=\"%d\", delimiter=\" \")\n\n        # Write size of matrix B\n        f.write(f\"{k} {n}\\n\")\n\n        # Generate and write matrix B (random integers between 0 and 100)\n        B = np.random.randint(0, 101, size=(k, n))\n        np.savetxt(f, B, fmt=\"%d\", delimiter=\" \")\n\n# Define matrix sizes\nmatrix_sizes = [100, 500, 1000, 2000, 5000, 8000, 10000]\n\n# Generate input files\nfor size in matrix_sizes:\n    filename = f\"Matrix_{size}x{size}.INP\"\n    print(f\"Generating {filename} ...\")\n    generate_matrix_input_file(filename, size, size, size)\n\nprint(\"\\nAll matrix input files generated successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T10:01:55.126470Z","iopub.execute_input":"2025-03-07T10:01:55.126758Z","iopub.status.idle":"2025-03-07T10:02:58.117356Z","shell.execute_reply.started":"2025-03-07T10:01:55.126736Z","shell.execute_reply":"2025-03-07T10:02:58.116597Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CPU","metadata":{}},{"cell_type":"code","source":"# Imports\nimport random\nimport numpy as np\nimport cudf\nimport cupy as cp\nimport cython\nfrom numba import cuda, int32\nfrom timeit import default_timer as timer\nfrom scipy.sparse import lil_matrix\nfrom pyspark.sql import SparkSession\nfrom pyspark.mllib.linalg.distributed import *\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T10:03:14.336845Z","iopub.execute_input":"2025-03-07T10:03:14.337131Z","iopub.status.idle":"2025-03-07T10:03:20.661734Z","shell.execute_reply.started":"2025-03-07T10:03:14.337112Z","shell.execute_reply":"2025-03-07T10:03:20.661070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%load_ext cython","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T10:03:23.104274Z","iopub.execute_input":"2025-03-07T10:03:23.104721Z","iopub.status.idle":"2025-03-07T10:03:23.602732Z","shell.execute_reply.started":"2025-03-07T10:03:23.104696Z","shell.execute_reply":"2025-03-07T10:03:23.602081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%cython\nimport numpy as np\ncimport numpy as np\ncimport cython\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ncdef np.ndarray[np.int32_t, ndim=2] _naive_dot(np.ndarray[np.int32_t, ndim=2] a, np.ndarray[np.int32_t, ndim=2] b):\n    cdef np.ndarray[np.int32_t, ndim=2] c\n    cdef int n, p, m\n    cdef np.int32_t s\n    if a.shape[1] != b.shape[0]:\n        raise ValueError('shape not matched')\n    n, p, m = a.shape[0], a.shape[1], b.shape[1]\n    c = np.zeros((n, m), dtype=np.int32)\n    for i in range(n):  # Use range instead of xrange for Python 3 compatibility\n        for j in range(m):\n            s = 0\n            for k in range(p):\n                s += a[i, k] * b[k, j]\n            c[i, j] = s\n    return c\n\ndef cpu_matrix_mul(a, b):\n    return _naive_dot(a, b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T10:03:25.834297Z","iopub.execute_input":"2025-03-07T10:03:25.834639Z","iopub.status.idle":"2025-03-07T10:03:27.954573Z","shell.execute_reply.started":"2025-03-07T10:03:25.834614Z","shell.execute_reply":"2025-03-07T10:03:27.953631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport time\n\ndef read_matrix_from_file(filename):\n    \"\"\"\n    Đọc ma trận từ file và trả về hai ma trận NumPy.\n    \"\"\"\n    with open(filename, \"r\") as f:\n        # Đọc kích thước ma trận A\n        m, k = map(int, f.readline().split())\n        matrix_A = np.array([list(map(int, f.readline().split())) for _ in range(m)], dtype=np.int32)\n\n        # Đọc kích thước ma trận B\n        k_check, n = map(int, f.readline().split())\n        assert k == k_check, \"Kích thước ma trận không hợp lệ!\"\n\n        matrix_B = np.array([list(map(int, f.readline().split())) for _ in range(k)], dtype=np.int32)\n\n    return matrix_A, matrix_B\n\n# List of dataset filenames\ndatasets = [\n    \"Matrix_100x100.INP\",\n    \"Matrix_500x500.INP\",\n    \"Matrix_1000x1000.INP\",\n    \"Matrix_2000x2000.INP\",\n    \"Matrix_5000x5000.INP\",\n    \"Matrix_8000x8000.INP\",\n    \"Matrix_10000x10000.INP\"\n]\n\nfor dataset in datasets:\n    A, B = read_matrix_from_file(dataset)\n    \n    # Measure execution time in milliseconds\n    start_time = time.time()\n    C = cpu_matrix_mul(A, B)\n    end_time = time.time()\n    \n    execution_time_ms = (end_time - start_time) * 1000  # Convert to milliseconds\n    print(f\"Execution time for {dataset}: {execution_time_ms:.3f} ms\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T10:03:34.022279Z","iopub.execute_input":"2025-03-07T10:03:34.022577Z","iopub.status.idle":"2025-03-07T11:10:18.054951Z","shell.execute_reply.started":"2025-03-07T10:03:34.022557Z","shell.execute_reply":"2025-03-07T11:10:18.054002Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cuda","metadata":{}},{"cell_type":"code","source":"%%writefile matrixMultiCuda.cu\n#include <iostream>\n#include <fstream>       \n#include <cuda_runtime.h>\n#define INPUT_FILE \"Matrix_8000x8000.INP\"\n#define OUTPUT_FILE \"MATRIX_RESULT.OUT\"\nusing namespace std;\n\n#define THREADS 32\n#define MAX_BLOCKS(size) ((size-1)/THREADS + 1)\n\n// Matrix multiplication kernel\n__global__ void matrixMultiply(float* A, float* B, float* C,\n                              int numARows, int numAColumns,\n                              int numBRows, int numBColumns,\n                              int numCRows, int numCColumns) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if ((row < numCRows) && (col < numCColumns)) {\n        float value = 0;\n        #pragma unroll\n        for (int k = 0; k < numAColumns; ++k) {\n            value += A[row * numAColumns + k] * B[k * numBColumns + col];\n        }\n        C[row * numCColumns + col] = value;\n    }\n}\n\nint main(int argc, char** argv) {\n    float *hostA, *hostB, *hostC;  // The A, B, and C (output) matrices\n    float *deviceA, *deviceB, *deviceC;\n    int numARows, numAColumns;    // dimensions of matrix A\n    int numBRows, numBColumns;    // dimensions of matrix B\n    int numCRows, numCColumns;    // dimensions of matrix C\n    \n    // Create CUDA events for timing\n    cudaEvent_t start, stop, start_total, stop_total;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n    cudaEventCreate(&start_total);\n    cudaEventCreate(&stop_total);\n    float milliseconds, total_time;\n\n    \n\n    /* Importing data and creating memory on host */\n    ifstream finp(INPUT_FILE);\n    cudaEventRecord(start);\n    \n    // Read dimensions of matrix A\n    finp >> numARows >> numAColumns;\n    int byteSizeA = sizeof(float) * numARows * numAColumns;\n    hostA = (float*)malloc(byteSizeA);\n        \n    // Read matrix A\n    for(int i = 0; i < numARows; i++) {\n        for(int j = 0; j < numAColumns; j++) {\n            finp >> hostA[i * numAColumns + j];\n        }\n    }\n\n    // Read dimensions of matrix B\n    finp >> numBRows >> numBColumns;\n    int byteSizeB = sizeof(float) * numBRows * numBColumns;\n    hostB = (float*)malloc(byteSizeB);\n    \n    // Read matrix B\n    for(int i = 0; i < numBRows; i++) {\n        for(int j = 0; j < numBColumns; j++) {\n            finp >> hostB[i * numBColumns + j];\n        }\n    }\n\n    // Set dimensions of output matrix C\n    numCRows = numARows;\n    numCColumns = numBColumns;\n    int byteSizeC = sizeof(float) * numCRows * numCColumns;\n    hostC = (float*)malloc(byteSizeC);\n\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"Time for importing data and creating host memory: \" << milliseconds << \" ms\" << endl;\n    \n    cout << \"Matrix Dimensions:\" << endl;\n    cout << \"A: \" << numARows << \" x \" << numAColumns << endl;\n    cout << \"B: \" << numBRows << \" x \" << numBColumns << endl;\n    cout << \"C: \" << numCRows << \" x \" << numCColumns << endl;\n\n    /* Start measuring total time */\n    cudaEventRecord(start_total);\n    \n    /* Allocating GPU memory */\n    cudaEventRecord(start);\n    cudaMalloc((void**)&deviceA, byteSizeA);\n    cudaMalloc((void**)&deviceB, byteSizeB);\n    cudaMalloc((void**)&deviceC, byteSizeC);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"Time for allocating GPU memory: \" << milliseconds << \" ms\" << endl;\n\n    /* Copying input memory to the GPU */\n    cudaEventRecord(start);\n    cudaMemcpy(deviceA, hostA, byteSizeA, cudaMemcpyHostToDevice);\n    cudaMemcpy(deviceB, hostB, byteSizeB, cudaMemcpyHostToDevice);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"Time to copy data from CPU to GPU: \" << milliseconds << \" ms\" << endl;\n\n    /* Initialize the grid and block dimensions */\n    dim3 blockDim(THREADS, THREADS);\n    dim3 gridDim(MAX_BLOCKS(numCColumns), MAX_BLOCKS(numCRows));\n\n    /* Launch the GPU Kernel */\n    cudaEventRecord(start);\n    matrixMultiply<<<gridDim, blockDim>>>(deviceA, deviceB, deviceC,\n                                         numARows, numAColumns,\n                                         numBRows, numBColumns,\n                                         numCRows, numCColumns);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"Time for kernel computation on GPU: \" << milliseconds << \" ms\" << endl;\n\n    /* Copying output memory to the CPU */\n    cudaEventRecord(start);\n    cudaMemcpy(hostC, deviceC, byteSizeC, cudaMemcpyDeviceToHost);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"Time to copy data from GPU to CPU: \" << milliseconds << \" ms\" << endl;\n\n    /* End measuring total time */\n    cudaEventRecord(stop_total);\n    cudaEventSynchronize(stop_total);\n    cudaEventElapsedTime(&total_time, start_total, stop_total);\n    cout << \"Total time: \" << total_time << \" ms\" << endl;\n\n    /* Output results */\n    ofstream fout(OUTPUT_FILE);\n    fout << numCRows << \" \" << numCColumns << endl;\n    for(int i = 0; i < numCRows; i++) {\n        for(int j = 0; j < numCColumns; j++) {\n            fout << hostC[i * numCColumns + j] << \" \";\n        }\n        fout << endl;\n    }\n    fout.close();\n\n    /* Freeing GPU Memory */\n    cudaFree(deviceA);\n    cudaFree(deviceB);\n    cudaFree(deviceC);\n\n    /* Freeing Host Memory */\n    free(hostA);\n    free(hostB);\n    free(hostC);\n\n    /* Destroy CUDA events */\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n    cudaEventDestroy(start_total);\n    cudaEventDestroy(stop_total);\n\n    return 0;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T11:19:13.730097Z","iopub.execute_input":"2025-03-07T11:19:13.730496Z","iopub.status.idle":"2025-03-07T11:19:13.737307Z","shell.execute_reply.started":"2025-03-07T11:19:13.730470Z","shell.execute_reply":"2025-03-07T11:19:13.736576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvcc matrixMultiCuda.cu -o matrixMultiCuda","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T11:19:18.026480Z","iopub.execute_input":"2025-03-07T11:19:18.026777Z","iopub.status.idle":"2025-03-07T11:19:20.225296Z","shell.execute_reply.started":"2025-03-07T11:19:18.026757Z","shell.execute_reply":"2025-03-07T11:19:20.224096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!./matrixMultiCuda","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T11:19:23.814124Z","iopub.execute_input":"2025-03-07T11:19:23.814522Z","iopub.status.idle":"2025-03-07T11:20:25.733426Z","shell.execute_reply.started":"2025-03-07T11:19:23.814494Z","shell.execute_reply":"2025-03-07T11:20:25.732494Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cuda Shared Memory","metadata":{}},{"cell_type":"code","source":"%%writefile matrixMultiSharedMemory.cu\n#include <iostream>\n#include <fstream>       \n#include <cuda_runtime.h>\n#define INPUT_FILE \"Matrix_8000x8000.INP\"\n#define OUTPUT_FILE \"LAB02.OUT\"\nusing namespace std;\n\n#define TILE_WIDTH 32\n#define THREADS TILE_WIDTH\n#define MAX_BLOCKS(size) ((size-1)/THREADS + 1)\n\n// Matrix multiplication kernel using shared memory\n__global__ void matrixMultiplyShared(float* A, float* B, float* C,\n                                    int numARows, int numAColumns,\n                                    int numBRows, int numBColumns,\n                                    int numCRows, int numCColumns) {\n    __shared__ float sharedA[TILE_WIDTH][TILE_WIDTH];\n    __shared__ float sharedB[TILE_WIDTH][TILE_WIDTH];\n    \n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    \n    // Global row and column indices\n    int row = by * TILE_WIDTH + ty;\n    int col = bx * TILE_WIDTH + tx;\n    \n    float value = 0;\n    \n    // Loop over tiles\n    for (int t = 0; t < (numAColumns-1)/TILE_WIDTH + 1; ++t) {\n        // Load data into shared memory\n        if (row < numARows && (t*TILE_WIDTH + tx) < numAColumns)\n            sharedA[ty][tx] = A[row * numAColumns + t * TILE_WIDTH + tx];\n        else\n            sharedA[ty][tx] = 0.0f;\n            \n        if ((t*TILE_WIDTH + ty) < numBRows && col < numBColumns)\n            sharedB[ty][tx] = B[(t * TILE_WIDTH + ty) * numBColumns + col];\n        else\n            sharedB[ty][tx] = 0.0f;\n            \n        __syncthreads();\n        \n        // Compute partial dot product within tile\n        #pragma unroll\n        for (int k = 0; k < TILE_WIDTH; ++k) {\n            value += sharedA[ty][k] * sharedB[k][tx];\n        }\n        __syncthreads();\n    }\n    \n    // Write result\n    if (row < numCRows && col < numCColumns) {\n        C[row * numCColumns + col] = value;\n    }\n}\n\nint main(int argc, char** argv) {\n    float *hostA, *hostB, *hostC;\n    float *deviceA, *deviceB, *deviceC;\n    int numARows, numAColumns;\n    int numBRows, numBColumns;\n    int numCRows, numCColumns;\n    \n    cudaEvent_t start, stop, start_total, stop_total;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n    cudaEventCreate(&start_total);\n    cudaEventCreate(&stop_total);\n    float milliseconds, total_time;\n\n    /* Importing data and creating memory on host */\n    ifstream finp(INPUT_FILE);\n    cudaEventRecord(start);\n    \n    finp >> numARows >> numAColumns;\n    int byteSizeA = sizeof(float) * numARows * numAColumns;\n    hostA = (float*)malloc(byteSizeA);\n        \n    for(int i = 0; i < numARows; i++) {\n        for(int j = 0; j < numAColumns; j++) {\n            finp >> hostA[i * numAColumns + j];\n        }\n    }\n\n    finp >> numBRows >> numBColumns;\n    int byteSizeB = sizeof(float) * numBRows * numBColumns;\n    hostB = (float*)malloc(byteSizeB);\n    \n    for(int i = 0; i < numBRows; i++) {\n        for(int j = 0; j < numBColumns; j++) {\n            finp >> hostB[i * numBColumns + j];\n        }\n    }\n\n    numCRows = numARows;\n    numCColumns = numBColumns;\n    int byteSizeC = sizeof(float) * numCRows * numCColumns;\n    hostC = (float*)malloc(byteSizeC);\n\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"Time for importing data and creating host memory: \" << milliseconds << \" ms\" << endl;\n    \n    cout << \"Matrix Dimensions:\" << endl;\n    cout << \"A: \" << numARows << \" x \" << numAColumns << endl;\n    cout << \"B: \" << numBRows << \" x \" << numBColumns << endl;\n    cout << \"C: \" << numCRows << \" x \" << numCColumns << endl;\n\n    cudaEventRecord(start_total);\n    \n    cudaEventRecord(start);\n    cudaMalloc((void**)&deviceA, byteSizeA);\n    cudaMalloc((void**)&deviceB, byteSizeB);\n    cudaMalloc((void**)&deviceC, byteSizeC);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"Time for allocating GPU memory: \" << milliseconds << \" ms\" << endl;\n\n    cudaEventRecord(start);\n    cudaMemcpy(deviceA, hostA, byteSizeA, cudaMemcpyHostToDevice);\n    cudaMemcpy(deviceB, hostB, byteSizeB, cudaMemcpyHostToDevice);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"Time to copy data from CPU to GPU: \" << milliseconds << \" ms\" << endl;\n\n    dim3 blockDim(TILE_WIDTH, TILE_WIDTH);\n    dim3 gridDim(MAX_BLOCKS(numCColumns), MAX_BLOCKS(numCRows));\n\n    cudaEventRecord(start);\n    matrixMultiplyShared<<<gridDim, blockDim>>>(deviceA, deviceB, deviceC,\n                                               numARows, numAColumns,\n                                               numBRows, numBColumns,\n                                               numCRows, numCColumns);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"Time for kernel computation on GPU: \" << milliseconds << \" ms\" << endl;\n\n    cudaEventRecord(start);\n    cudaMemcpy(hostC, deviceC, byteSizeC, cudaMemcpyDeviceToHost);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n    cudaEventElapsedTime(&milliseconds, start, stop);\n    cout << \"Time to copy data from GPU to CPU: \" << milliseconds << \" ms\" << endl;\n\n    cudaEventRecord(stop_total);\n    cudaEventSynchronize(stop_total);\n    cudaEventElapsedTime(&total_time, start_total, stop_total);\n    cout << \"Total time: \" << total_time << \" ms\" << endl;\n\n    ofstream fout(OUTPUT_FILE);\n    fout << numCRows << \" \" << numCColumns << endl;\n    for(int i = 0; i < numCRows; i++) {\n        for(int j = 0; j < numCColumns; j++) {\n            fout << hostC[i * numCColumns + j] << \" \";\n        }\n        fout << endl;\n    }\n    fout.close();\n\n    cudaFree(deviceA);\n    cudaFree(deviceB);\n    cudaFree(deviceC);\n\n    free(hostA);\n    free(hostB);\n    free(hostC);\n\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n    cudaEventDestroy(start_total);\n    cudaEventDestroy(stop_total);\n\n    return 0;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T11:23:44.518963Z","iopub.execute_input":"2025-03-07T11:23:44.519333Z","iopub.status.idle":"2025-03-07T11:23:44.526104Z","shell.execute_reply.started":"2025-03-07T11:23:44.519304Z","shell.execute_reply":"2025-03-07T11:23:44.525226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvcc matrixMultiSharedMemory.cu -o matrixMultiSharedMemory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T11:23:48.665153Z","iopub.execute_input":"2025-03-07T11:23:48.665495Z","iopub.status.idle":"2025-03-07T11:23:50.754999Z","shell.execute_reply.started":"2025-03-07T11:23:48.665471Z","shell.execute_reply":"2025-03-07T11:23:50.753888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!./matrixMultiSharedMemory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T11:23:51.800491Z","iopub.execute_input":"2025-03-07T11:23:51.800811Z","iopub.status.idle":"2025-03-07T11:24:53.212732Z","shell.execute_reply.started":"2025-03-07T11:23:51.800786Z","shell.execute_reply":"2025-03-07T11:24:53.211754Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verison Cupy ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cupy as cp\nimport time\n\n# Define the CUDA kernel as a string\nmatrix_multiply_kernel = r'''\nextern \"C\" __global__ void matrixMultiplyShared(\n    const float* A, const float* B, float* C,\n    int numARows, int numAColumns,\n    int numBRows, int numBColumns,\n    int numCRows, int numCColumns) {\n    \n    __shared__ float sharedA[32][32];\n    __shared__ float sharedB[32][32];\n    \n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    \n    int row = by * 32 + ty;\n    int col = bx * 32 + tx;\n    \n    float value = 0;\n    \n    for (int t = 0; t < (numAColumns-1)/32 + 1; ++t) {\n        if (row < numARows && (t*32 + tx) < numAColumns)\n            sharedA[ty][tx] = A[row * numAColumns + t * 32 + tx];\n        else\n            sharedA[ty][tx] = 0.0f;\n            \n        if ((t*32 + ty) < numBRows && col < numBColumns)\n            sharedB[ty][tx] = B[(t * 32 + ty) * numBColumns + col];\n        else\n            sharedB[ty][tx] = 0.0f;\n            \n        __syncthreads();\n        \n        #pragma unroll\n        for (int k = 0; k < 32; ++k) {\n            value += sharedA[ty][k] * sharedB[k][tx];\n        }\n        __syncthreads();\n    }\n    \n    if (row < numCRows && col < numCColumns) {\n        C[row * numCColumns + col] = value;\n    }\n}\n'''\n\ndef matrix_multiply(A, B):\n    # Ensure inputs are CuPy arrays\n    if isinstance(A, np.ndarray):\n        A = cp.asarray(A)\n    if isinstance(B, np.ndarray):\n        B = cp.asarray(B)\n    \n    # Get matrix dimensions\n    numARows, numAColumns = A.shape\n    numBRows, numBColumns = B.shape\n    \n    # Validate matrix dimensions\n    if numAColumns != numBRows:\n        raise ValueError(\"Matrix dimensions do not match for multiplication\")\n    \n    numCRows = numARows\n    numCColumns = numBColumns\n    \n    # Allocate output matrix\n    C = cp.zeros((numCRows, numCColumns), dtype=cp.float32)\n    \n    # Compile the CUDA kernel\n    kernel = cp.RawKernel(matrix_multiply_kernel, 'matrixMultiplyShared')\n    \n    # Define grid and block dimensions\n    threads_per_block = 32\n    grid_x = (numCColumns - 1) // threads_per_block + 1\n    grid_y = (numCRows - 1) // threads_per_block + 1\n    \n    # Time measurement\n    start = time.time()\n    \n    # Launch kernel\n    kernel(\n        (grid_x, grid_y),\n        (threads_per_block, threads_per_block),\n        (A, B, C, numARows, numAColumns, numBRows, numBColumns, numCRows, numCColumns)\n    )\n    \n    cp.cuda.stream.get_current_stream().synchronize()\n    end = time.time()\n    \n    print(f\"Kernel execution time: {(end - start) * 1000:.2f} ms\")\n    \n    return C\n\n# Example usage\nif __name__ == \"__main__\":\n    # Read input matrices from file\n    with open(\"Matrix_8000x8000.INP\", \"r\") as f:\n        # Read matrix A\n        numARows, numAColumns = map(int, f.readline().split())\n        A = np.zeros((numARows, numAColumns), dtype=np.float32)\n        for i in range(numARows):\n            A[i] = list(map(float, f.readline().split()))\n        \n        # Read matrix B\n        numBRows, numBColumns = map(int, f.readline().split())\n        B = np.zeros((numBRows, numBColumns), dtype=np.float32)\n        for i in range(numBRows):\n            B[i] = list(map(float, f.readline().split()))\n    \n    print(f\"Matrix A shape: {A.shape}\")\n    print(f\"Matrix B shape: {B.shape}\")\n    \n    # Perform matrix multiplication\n    start_total = time.time()\n    \n    # Transfer to GPU\n    start = time.time()\n    A_gpu = cp.asarray(A)\n    B_gpu = cp.asarray(B)\n    cp.cuda.stream.get_current_stream().synchronize()\n    end = time.time()\n    print(f\"Time to copy data to GPU: {(end - start) * 1000:.3f} ms\")\n    \n    # Multiply matrices\n    C_gpu = matrix_multiply(A_gpu, B_gpu)\n    \n    # Transfer result back to CPU\n    start = time.time()\n    C = cp.asnumpy(C_gpu)\n    cp.cuda.stream.get_current_stream().synchronize()\n    end = time.time()\n    print(f\"Time to copy data from GPU: {(end - start) * 1000:.3f} ms\")\n    \n    end_total = time.time()\n    print(f\"Total time: {(end_total - start_total) * 1000:.3f} ms\")\n\n    # Tính kết quả trên CPU bằng NumPy\n    C_numpy = np.dot(A, B)\n\n    # So sánh kết quả giữa GPU và CPU\n    if np.allclose(C, C_numpy, atol=1e-5):\n        print(\"Kết quả chính xác! GPU và CPU cho cùng một kết quả.\")\n    else:\n        print(\"Kết quả không khớp! Kiểm tra lại phép nhân ma trận hoặc kernel CUDA.\")\n    \n    # Save result to file\n    with open(\"LAB02.OUT\", \"w\") as f:\n        f.write(f\"{C.shape[0]} {C.shape[1]}\\n\")\n        for row in C:\n            f.write(\" \".join(map(str, row)) + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T11:30:38.327520Z","iopub.execute_input":"2025-03-07T11:30:38.327913Z","iopub.status.idle":"2025-03-07T11:31:34.820974Z","shell.execute_reply.started":"2025-03-07T11:30:38.327883Z","shell.execute_reply":"2025-03-07T11:31:34.819972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cupy as cp\nimport time\n\n# Define the CUDA kernel as a string\nmatrix_multiply_kernel = r'''\nextern \"C\" __global__ void matrixMultiplyShared(\n    const float* A, const float* B, float* C,\n    int numARows, int numAColumns,\n    int numBRows, int numBColumns,\n    int numCRows, int numCColumns) {\n    \n    __shared__ float sharedA[32][32];\n    __shared__ float sharedB[32][32];\n    \n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    \n    int row = by * 32 + ty;\n    int col = bx * 32 + tx;\n    \n    float value = 0;\n    \n    for (int t = 0; t < (numAColumns-1)/32 + 1; ++t) {\n        if (row < numARows && (t*32 + tx) < numAColumns)\n            sharedA[ty][tx] = A[row * numAColumns + t * 32 + tx];\n        else\n            sharedA[ty][tx] = 0.0f;\n            \n        if ((t*32 + ty) < numBRows && col < numBColumns)\n            sharedB[ty][tx] = B[(t * 32 + ty) * numBColumns + col];\n        else\n            sharedB[ty][tx] = 0.0f;\n            \n        __syncthreads();\n        \n        #pragma unroll\n        for (int k = 0; k < 32; ++k) {\n            value += sharedA[ty][k] * sharedB[k][tx];\n        }\n        __syncthreads();\n    }\n    \n    if (row < numCRows && col < numCColumns) {\n        C[row * numCColumns + col] = value;\n    }\n}\n'''\n\ndef matrix_multiply(A, B):\n    # Ensure inputs are CuPy arrays\n    if isinstance(A, np.ndarray):\n        A = cp.asarray(A)\n    if isinstance(B, np.ndarray):\n        B = cp.asarray(B)\n    \n    # Get matrix dimensions\n    numARows, numAColumns = A.shape\n    numBRows, numBColumns = B.shape\n    \n    # Validate matrix dimensions\n    if numAColumns != numBRows:\n        raise ValueError(\"Matrix dimensions do not match for multiplication\")\n    \n    numCRows = numARows\n    numCColumns = numBColumns\n    \n    # Allocate output matrix (Measure time)\n    start_alloc = time.time()\n    C = cp.zeros((numCRows, numCColumns), dtype=cp.float32)\n    cp.cuda.stream.get_current_stream().synchronize()\n    end_alloc = time.time()\n    \n    # Compile the CUDA kernel\n    kernel = cp.RawKernel(matrix_multiply_kernel, 'matrixMultiplyShared')\n    \n    # Define grid and block dimensions\n    threads_per_block = 32\n    grid_x = (numCColumns - 1) // threads_per_block + 1\n    grid_y = (numCRows - 1) // threads_per_block + 1\n    \n    # Kernel computation time\n    start_kernel = time.time()\n    kernel(\n        (grid_x, grid_y),\n        (threads_per_block, threads_per_block),\n        (A, B, C, numARows, numAColumns, numBRows, numBColumns, numCRows, numCColumns)\n    )\n    cp.cuda.stream.get_current_stream().synchronize()\n    end_kernel = time.time()\n    \n    print(f\"Time for allocating GPU memory: {(end_alloc - start_alloc) * 1000:.2f} ms\")\n    print(f\"Kernel execution time: {(end_kernel - start_kernel) * 1000:.2f} ms\")\n    \n    return C\n\n# Example usage\nif __name__ == \"__main__\":\n    # Read input matrices from file\n    with open(\"LAB02_large.INP\", \"r\") as f:\n        # Read matrix A\n        numARows, numAColumns = map(int, f.readline().split())\n        A = np.zeros((numARows, numAColumns), dtype=np.float32)\n        for i in range(numARows):\n            A[i] = list(map(float, f.readline().split()))\n        \n        # Read matrix B\n        numBRows, numBColumns = map(int, f.readline().split())\n        B = np.zeros((numBRows, numBColumns), dtype=np.float32)\n        for i in range(numBRows):\n            B[i] = list(map(float, f.readline().split()))\n    \n    print(f\"Matrix A shape: {A.shape}\")\n    print(f\"Matrix B shape: {B.shape}\")\n    \n    # Total execution time measurement\n    start_total = time.time()\n    \n    # Copy data to GPU\n    start_copy_to_gpu = time.time()\n    A_gpu = cp.asarray(A)\n    B_gpu = cp.asarray(B)\n    cp.cuda.stream.get_current_stream().synchronize()\n    end_copy_to_gpu = time.time()\n    \n    print(f\"Time to copy data to GPU: {(end_copy_to_gpu - start_copy_to_gpu) * 1000:.2f} ms\")\n    \n    # Multiply matrices\n    C_gpu = matrix_multiply(A_gpu, B_gpu)\n    \n    # Copy result back to CPU\n    start_copy_from_gpu = time.time()\n    C = cp.asnumpy(C_gpu)\n    cp.cuda.stream.get_current_stream().synchronize()\n    end_copy_from_gpu = time.time()\n    \n    print(f\"Time to copy data from GPU: {(end_copy_from_gpu - start_copy_from_gpu) * 1000:.2f} ms\")\n    \n    end_total = time.time()\n    print(f\"Total execution time: {(end_total - start_total) * 1000:.2f} ms\")\n\n    # Verify correctness with NumPy\n    C_numpy = np.dot(A, B)\n\n    if np.allclose(C, C_numpy, atol=1e-5):\n        print(\"✅ GPU and CPU results match!\")\n    else:\n        print(\"❌ GPU and CPU results do not match. Please check.\")\n\n    # Save result to file\n    with open(\"LAB02.OUT\", \"w\") as f:\n        f.write(f\"{C.shape[0]} {C.shape[1]}\\n\")\n        for row in C:\n            f.write(\" \".join(map(str, row)) + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T06:41:31.814257Z","iopub.execute_input":"2025-03-05T06:41:31.814635Z","iopub.status.idle":"2025-03-05T06:41:46.754544Z","shell.execute_reply.started":"2025-03-05T06:41:31.814592Z","shell.execute_reply":"2025-03-05T06:41:46.753322Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Rapids trên Kernel Cupy","metadata":{}},{"cell_type":"code","source":"import cupy as cp\nimport cudf\nimport time\n\n# CUDA Kernel for Matrix Multiplication\nmatrix_mult_kernel = cp.RawKernel(r'''\nextern \"C\" __global__ void matmul(\n    float* A, float* B, float* C, int N, int M, int P) {\n    \n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < N && col < P) {\n        float sum = 0;\n        for (int k = 0; k < M; k++) {\n            sum += A[row * M + k] * B[k * P + col];\n        }\n        C[row * P + col] = sum;\n    }\n}\n''', 'matmul')\n\n# Custom Matrix Multiplication Function\ndef matmul(self, other):\n    if not isinstance(other, cudf.DataFrame):\n        raise ValueError(\"Matrix multiplication requires another cuDF DataFrame\")\n\n    # Start total execution time measurement\n    total_start = time.time()\n\n    # 1. Time for Memory Allocation (GPU)\n    start_alloc = time.time()\n    A_gpu = cp.empty(self.shape, dtype=cp.float32)\n    B_gpu = cp.empty(other.shape, dtype=cp.float32)\n    C_gpu = cp.empty((self.shape[0], other.shape[1]), dtype=cp.float32)\n    end_alloc = time.time()\n\n    # 2. Time for CPU → GPU Copy\n    start_copy_h2d = time.time()\n    A_gpu[:] = cp.array(self.values, dtype=cp.float32)\n    B_gpu[:] = cp.array(other.values, dtype=cp.float32)\n    end_copy_h2d = time.time()\n\n    # Matrix dimensions\n    N, M = A_gpu.shape\n    M_B, P = B_gpu.shape\n    if M != M_B:\n        raise ValueError(\"Matrix dimensions do not match for multiplication\")\n\n    # 3. Time for Kernel Execution\n    threads_per_block = (16, 16)\n    blocks_per_grid = ((P + 15) // 16, (N + 15) // 16)\n    start_kernel = time.time()\n    matrix_mult_kernel(blocks_per_grid, threads_per_block, (A_gpu, B_gpu, C_gpu, N, M, P))\n    cp.cuda.Device(0).synchronize()  # Ensure GPU computation finishes\n    end_kernel = time.time()\n\n    # 4. Time for GPU → CPU Copy\n    start_copy_d2h = time.time()\n    result = C_gpu.get()\n    end_copy_d2h = time.time()\n\n    # End total execution time measurement\n    total_end = time.time()\n\n    # Convert back to cuDF DataFrame\n    result_df = cudf.DataFrame(result)\n\n    # Convert all times to milliseconds\n    alloc_time = (end_alloc - start_alloc) * 1000\n    copy_h2d_time = (end_copy_h2d - start_copy_h2d) * 1000\n    kernel_time = (end_kernel - start_kernel) * 1000\n    copy_d2h_time = (end_copy_d2h - start_copy_d2h) * 1000\n    total_time = (total_end - total_start) * 1000\n\n    # Print Timing Information\n    print(f\"Memory Allocation Time: {alloc_time:.3f} ms\")\n    print(f\"CPU → GPU Copy Time: {copy_h2d_time:.3f} ms\")\n    print(f\"Kernel Execution Time: {kernel_time:.3f} ms\")\n    print(f\"GPU → CPU Copy Time: {copy_d2h_time:.3f} ms\")\n    print(f\"Total Execution Time: {total_time:.3f} ms\")\n    \n    return result_df\n\n# Monkey-patch cuDF DataFrame to add the custom method\ncudf.DataFrame.matmul = matmul\n\n# Function to read matrix from file\ndef read_matrix_from_file(filename):\n    \"\"\"\n    Đọc ma trận từ file và trả về 2 ma trận A, B dưới dạng cuDF DataFrame.\n    \"\"\"\n    with open(filename, \"r\") as f:\n        # Read matrix A dimensions\n        m, k = map(int, f.readline().split())\n        matrix_A = [list(map(float, f.readline().split())) for _ in range(m)]\n        \n        # Read matrix B dimensions\n        k_check, n = map(int, f.readline().split())\n        assert k == k_check, \"Matrix dimensions do not match!\"\n        \n        matrix_B = [list(map(float, f.readline().split())) for _ in range(k)]\n    \n    # Convert to cuDF DataFrame\n    df_A = cudf.DataFrame(matrix_A)\n    df_B = cudf.DataFrame(matrix_B)\n    \n    return df_A, df_B, m, n\n\n# Function to execute matrix multiplication and print execution time\ndef run_rapids_matrix_multiplication(filename):\n    print(f\"\\nProcessing {filename} ...\")\n    A_df, B_df, _, _ = read_matrix_from_file(filename)\n\n    # Perform matrix multiplication using RAPIDS\n    result_df = A_df.matmul(B_df)\n\n    print(\"Result (First 5 Rows):\")\n    print(result_df.head())\n\n# List of test datasets\ndatasets = [\n    \"Matrix_100x100.INP\",\n    \"Matrix_500x500.INP\",\n    \"Matrix_1000x1000.INP\",\n    \"Matrix_2000x2000.INP\",\n    \"Matrix_5000x5000.INP\",\n    \"Matrix_8000x8000.INP\",\n    \"Matrix_10000x10000.INP\"\n]\n\n# Run on all datasets\nfor dataset in datasets:\n    run_rapids_matrix_multiplication(dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T11:32:54.285575Z","iopub.execute_input":"2025-03-07T11:32:54.285911Z","iopub.status.idle":"2025-03-07T11:37:26.037474Z","shell.execute_reply.started":"2025-03-07T11:32:54.285887Z","shell.execute_reply":"2025-03-07T11:37:26.036648Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ","metadata":{}}]}